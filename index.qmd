---
title: "Selectivity Options for Age-Structured Stock Assessments"
subtitle: "A comparative framework using RTMB"
author:
  - name: James Ianelli
    affiliations:
      - name: NOAA/NMFS Alaska Fisheries Science Center
        city: Seattle
        state: WA
date: last-modified
abstract: |
  Selectivity is a fundamental component of age-structured stock assessment models,
  governing how fishing mortality and survey catchability vary with age or size. The
  choice of selectivity parameterization affects estimates of population abundance,
  fishing mortality, and management reference points. This manuscript compares several
  selectivity formulations---including standard logistic, double logistic,
  spline-based, and time-varying 2D autoregressive approaches---within a unified
  RTMB framework. We examine parameter
  identifiability, prior sensitivity, and the practical consequences of selectivity
  misspecification using simulation and application to eastern Bering Sea walleye
  pollock (*Gadus chalcogrammus*). Results highlight trade-offs between flexibility
  and estimability and provide guidance for practitioners choosing among selectivity
  options in operational assessments.
keywords:
  - selectivity
  - stock assessment
  - RTMB
  - fisheries
  - walleye pollock
---

## Introduction {#sec-intro}

Selectivity functions describe the relative vulnerability of fish to capture as a
function of age (or size) and are among the most influential---yet least
observable---components of stock assessment models. The assumed shape of the
selectivity curve directly affects estimates of spawning biomass, recruitment, and
fishing mortality, and misspecification can propagate into biased reference points
and management advice [@punt2013; @thompson1994].

Despite its importance, selectivity is often treated as a modelling convenience
rather than a biological or technological quantity to be estimated carefully. Most
operational assessments adopt a logistic or double-logistic functional form, chosen
for parsimony rather than fidelity to the underlying catch process. More flexible
alternatives---such as penalized splines or non-parametric approaches---can reduce
structural bias but may introduce identifiability problems, particularly when data
are sparse or conflicting.

This manuscript develops a comparative framework for evaluating selectivity options
within the R Template Model Builder (RTMB) environment. RTMB provides automatic
differentiation, Laplace approximation for random effects, and integration with
Bayesian sampling via SparseNUTS, making it a natural platform for exploring both
maximum likelihood and posterior-based inference on selectivity parameters.

We organize the comparison around four selectivity families:

1. **Standard logistic** (@sec-logistic): the two-parameter ascending logistic,
   widely used for fisheries where retention is assumed to be monotonically
   increasing with age.
2. **Double logistic** (@sec-double-logistic): a dome-shaped three-parameter
   formulation allowing selectivity to decline at older ages, motivated by gear
   avoidance, ontogenetic habitat shifts, or differential availability.
3. **Spline-based** (@sec-spline): a penalized B-spline approach offering flexible,
   data-driven selectivity shapes with smoothness controlled by a penalty parameter.
4. **Time-varying 2D AR1** (@sec-timevarying): a separable autoregressive structure
   over year and age dimensions, allowing selectivity to evolve smoothly through
   time while maintaining age-specific coherence via a Kronecker-structured
   precision matrix.

For each family, we present the mathematical formulation, an RTMB implementation
with prior specification, parameter sensitivity analysis, and MCMC-based posterior
evaluation. We then apply all to eastern Bering Sea walleye pollock data to
illustrate practical trade-offs.

## Review Scope and Literature Context {#sec-scope}

This review focuses on methodological literature that is directly relevant to
three practical assessment decisions: (i) choice of selectivity functional form,
(ii) model selection among competing selectivity hypotheses, and (iii)
construction of time-varying selectivity processes with explicit regularization.

### Selectivity form choice in integrated assessments

Operationally, selectivity in integrated assessments is best interpreted as the
combined effect of gear contact/retention and fish availability, not as a purely
mechanistic gear curve [@punt2013; @priviterajohnson2022]. In that setting, the
choice of selectivity parameterization is consequential because it changes
inference on spawning biomass, recruitment, and fishing mortality.

Selectivity-form selection therefore requires both statistical and biological
diagnostics, including residual pattern checks, sensitivity analyses, and
information-based comparisons among plausible alternatives [@punt2013]. A key
warning is that non-monotone selectivity can be strongly confounded with natural
mortality, particularly when older ages are weakly informed by composition data
[@thompson1994].

### Gear-selectivity estimation foundations

Classical selectivity-estimation literature remains important for assessment
modeling because it clarifies what selectivity data can identify and where latent
availability effects remain unresolved [@millar1999_size_selection;
@wileman1996_ices_selectivity_manual]. This evidence base supports explicit
regularization whenever flexible age-specific selectivity surfaces are estimated.

### Time-varying selectivity literature

Simulation studies show that time-varying selectivity can materially improve or
degrade assessment performance depending on how process flexibility matches the
data-generating mechanism [@linton2011_timevarying]. Practitioner guidance also
emphasizes that subjective choices such as block boundaries, smoothing penalties,
and process assumptions can dominate outcomes [@martell2014_good_practices_tvselex].

Recent semi-parametric approaches formalize autocorrelation across both age and
time, motivating separable latent-process structures that can be estimated within
modern AD frameworks [@xu2019_semiparam_ar_selex]. A foundational applied example
is the likelihood-based Southern Bluefin Tuna assessment of Butterworth et al.
(2003), which allows temporally structured selectivity changes and treats
catch-at-age data as error-prone rather than fixed [@butterworth2003_sbt_selectivity].
Broader SCA model-selection and time-varying process work reinforces the same
bias-variance trade-off [@wilberg2006_timevarying_q; @wilberg2008_dic_sca].

## Statistical Framing as a Latent Process {#sec-latent}

Although selectivity is often presented as a deterministic curve, it is more
usefully viewed as a latent process that allocates fishing intensity over age and
time. Under this framing, selecting a functional form is equivalent to selecting a
prior precision structure on an unobserved selectivity surface.

Low-dimensional parametric forms (for example, logistic and double logistic)
impose strong structural priors and are robust when data are sparse. Flexible
forms (for example, spline and time-varying AR structures) reduce structural bias
risk, but require explicit regularization to prevent confounding and overfitting
[@martell2014_good_practices_tvselex; @xu2019_semiparam_ar_selex].

This perspective motivates three principles for applied assessments:

1. Match effective selectivity complexity to data information content.
2. Treat regularization (penalties/priors/precision structures) as essential, not optional.
3. Use model-selection tools (AIC, DIC, cross-validation, posterior predictive checks)
   to evaluate bias-variance trade-offs under explicit prior structure
   [@punt2013; @wilberg2008_dic_sca].

Regularization can also be framed as numerical stabilization: penalties such as
ridge/Tikhonov terms or spline roughness controls impose smoothness that improves
conditioning and mitigates overfitting, echoing the numerical recipes literature
on stabilization and controlled flexibility in nonlinear optimization
[@press1992_numerical_recipes_c].

Within this unifying view, logistic, dome-shaped, spline, blocked, and AR(1)
approaches differ primarily in the precision matrix implied for latent
selectivity deviations, rather than only in curve geometry.


## Standard Logistic Selectivity {#sec-logistic}

{{< embed logistic_selectivity.qmd >}}


## Double Logistic Selectivity {#sec-double-logistic}

{{< embed double_logistic_selectivity.qmd >}}


## Spline-Based Selectivity {#sec-spline}

{{< embed spline_selectivity.qmd >}}


## Time-Varying Selectivity with 2D Autoregressive Structure {#sec-timevarying}

{{< embed timevarying_selectivity.qmd >}}

### Comparison of time-varying selectivity approaches

```{r}
#| label: tbl-timevarying-compare
#| tbl-cap: "Comparison of time‑varying selectivity approaches in applied assessments."
compare_tv <- data.frame(
  Approach = c(
    "Structured temporal selectivity (Butterworth et al. 2003)",
    "Separable AR1 surface (this manuscript)",
    "Block/time‑period selectivity"
  ),
  Structure = c(
    "Pre‑specified temporal structure on selectivity changes",
    "Autocorrelated deviations across age and time",
    "Piecewise‑constant selectivity blocks"
  ),
  Strengths = c(
    "Historical applied example; likelihood‑based CAA with error",
    "Smooth evolution; explicit regularization",
    "Transparent and easy to communicate"
  ),
  Risks = c(
    "Potential confounding if structure is too rigid",
    "Over‑flexibility if penalties are weak",
    "Arbitrary block boundaries"
  ),
  check.names = FALSE
)

knitr::kable(compare_tv)
```

## Comparison and Discussion {#sec-discussion}

The four selectivity families evaluated here span a practical gradient from
low-dimensional structural assumptions to high-dimensional latent-process
flexibility. The logistic model (@sec-logistic; @fig-logistic-scenarios) provides
an interpretable baseline with strong monotonicity assumptions and minimal
parameter burden. The double-logistic model (@sec-double-logistic;
@tbl-dl-scenarios, @fig-dl-faceted, @fig-dl-sensitivity) adds biologically
plausible dome-shape behavior, but also increases confounding risk with natural
mortality and cohort effects when data support is limited [@thompson1994].

Spline selectivity (@sec-spline; @fig-spline-basis; @fig-spline-examples) offers
useful middle ground: flexible shape estimation with transparent control of
roughness via penalization. In review terms, this is a semi-parametric compromise
between rigid parametric forms and fully dynamic latent surfaces. The time-varying
2D AR1 approach (@sec-timevarying; @fig-tv-sim; @fig-tv-lines) further extends
flexibility by allowing coherent year-by-age evolution with explicit process
structure, aligning with recent recommendations for autocorrelated selectivity
deviations [@linton2011_timevarying; @xu2019_semiparam_ar_selex].

For scientific review and operational assessment use, a defensible workflow is:

1. Start with a parsimonious baseline (logistic or double logistic) and diagnose
   fit deficiencies in composition residuals and retrospective patterns.
2. Introduce additional flexibility (spline or time-varying structures) only when
   diagnostics indicate systematic lack of fit and data support richer structure.
3. Compare candidate models with complementary evidence, including information
   criteria, predictive checks, and sensitivity of management quantities
   [@punt2013; @wilberg2008_dic_sca].
4. Report regularization choices and identifiability diagnostics explicitly,
   because inferred selectivity dynamics are conditional on prior/penalty
   assumptions [@martell2014_good_practices_tvselex; @priviterajohnson2022].

Overall, the central trade-off is not whether one curve family is universally
"best", but whether the assumed latent structure is commensurate with the
available information and management objectives.


## References {.unnumbered}

::: {#refs}
:::
